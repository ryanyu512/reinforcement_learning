{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/r/Desktop/miniforge3/envs/tf_env/lib/python3.10/site-packages/gym/envs/registration.py:423: UserWarning: \u001b[33mWARN: Custom namespace `ALE` is being overridden by namespace `ALE`. If you are developing a plugin you shouldn't specify a namespace in `register` calls. The namespace is specified through the entry point package metadata.\u001b[0m\n",
      "  logger.warn(\n",
      "Warning: Gym version v0.24.0 has a number of critical issues with `gym.make` such that the `reset` and `step` functions are called before returning the environment. It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Evironment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/r/Desktop/miniforge3/envs/tf_env/lib/python3.10/site-packages/gym/envs/registration.py:568: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env._max_episode_steps = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Model):\n",
    "    def __init__(self, \n",
    "                 lr = 1e-3, \n",
    "                 gamma = 0.99,\n",
    "                 n_actions = 2, \n",
    "                 **kwargs):\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.p_net = self.FCN()\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.action_space = [_ for _ in range(n_actions)]\n",
    "        self.step = 0\n",
    "        \n",
    "        self.p_net_filename = 'p_net.h5'\n",
    "        \n",
    "    def compile(self, opt, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.opt = opt\n",
    "        \n",
    "    def FCN(self, n_inputs = 4, n_hiddens = [128], n_actions = 2):\n",
    "        model = Sequential()\n",
    "        model.add(tf.keras.Input(shape = (n_inputs,)))\n",
    "        model.add(Dense(units = n_hiddens[0], activation = \"relu\"))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(units = n_actions, activation = \"softmax\"))\n",
    "    \n",
    "        return model\n",
    "    \n",
    "    def select_action(self, observation, is_training = True):\n",
    "        state = tf.convert_to_tensor(np.array([observation], copy=False, dtype = np.float32))\n",
    "        if is_training:\n",
    "            p = self.p_net(state, training = True)\n",
    "        else:\n",
    "            p = self.p_net(state, training = False)\n",
    "        \n",
    "        tmp = p\n",
    "        tmp = tf.cast(tmp, dtype = tf.float64)\n",
    "        tmp = tmp*(1./tf.reduce_sum(tmp))\n",
    "        \n",
    "        a = np.random.choice(a = len(self.action_space), \n",
    "                                size = 1, \n",
    "                                replace = False, \n",
    "                                p = tmp[0,:])[0]\n",
    "        \n",
    "        log_p = tf.math.log(p[:, a])\n",
    "        log_p = tf.math.reduce_sum(log_p)\n",
    "\n",
    "        return a, log_p\n",
    "        \n",
    "    def learn(self, rewards, log_ps, g):\n",
    "        \n",
    "        d_rewards = np.array([0]*len(rewards))\n",
    "        for i in range(len(rewards)):\n",
    "            Gt = 0.\n",
    "            pw = 0\n",
    "            for r in rewards[i:]:\n",
    "                Gt += r*self.gamma**pw\n",
    "                pw += 1\n",
    "            d_rewards[i] = Gt\n",
    "\n",
    "        d_rewards = tf.convert_to_tensor(d_rewards, dtype = tf.float32)\n",
    "        d_rewards = (d_rewards - tf.math.reduce_mean(d_rewards))/(tf.math.reduce_std(d_rewards) + 1e-9)\n",
    "        \n",
    "        loss_fcn = []\n",
    "        for log_p, Gt in zip(log_ps, d_rewards):\n",
    "            loss_fcn.append(-log_p*Gt)\n",
    "\n",
    "        loss_fcn = tf.convert_to_tensor(loss_fcn, dtype = tf.float32)\n",
    "        loss_fcn = tf.reduce_sum(loss_fcn, 0)\n",
    "        \n",
    "        grad = g.gradient(loss_fcn, self.p_net.trainable_variables)\n",
    "        self.opt.apply_gradients(zip(grad, self.p_net.trainable_variables))\n",
    "               \n",
    "    def train(self, n_episodes = 500, n_trial = 500, windows = 20):\n",
    "        steps = 0\n",
    "        rewards_sum = []\n",
    "        rewards_avg = []\n",
    "        best_rewards = -np.inf\n",
    "        for episode in range(n_episodes):\n",
    "            \n",
    "            obs = env.reset()\n",
    "            log_ps = []\n",
    "            rewards = []\n",
    "            with tf.GradientTape() as tape:  \n",
    "                for i in range(n_trial):\n",
    "                    #select action\n",
    "                    action, log_p = self.select_action(obs)\n",
    "                    \n",
    "                    #interact with the environment\n",
    "                    obs_, reward, done, info = env.step(action)\n",
    "                    \n",
    "                    log_ps.append(log_p)\n",
    "                    rewards.append(reward)\n",
    "                    \n",
    "                    if done or i == n_trial - 1:\n",
    "                        #learn from experience\n",
    "                        self.learn(rewards = rewards, log_ps = log_ps, g = tape)\n",
    "\n",
    "                        rewards_sum.append(np.sum(rewards))\n",
    "                        rewards_avg.append(np.mean(rewards_sum[-windows:]))\n",
    "                        \n",
    "                        if rewards_avg[-1] > best_rewards:\n",
    "                            best_rewards = rewards_avg[-1]\n",
    "                            self.save_models()\n",
    "                        print(f\"episode: {episode}, rewards_sum: {rewards_sum[episode]}, rewards_avg: {rewards_avg[episode]}, best_rewards: {best_rewards}\")\n",
    "                        break\n",
    "                    obs = obs_\n",
    "            \n",
    "    def save_models(self):\n",
    "        self.p_net.save(self.p_net_filename)\n",
    "        print(\"... saving model ...\")\n",
    "        \n",
    "    def load_models(self):\n",
    "        self.p_net = load_model(self.p_net_filename)\n",
    "        print('... loading models ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 01:07:11.952654: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-11-08 01:07:11.952766: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "n_episode = 500\n",
    "n_trial = env._max_episode_steps\n",
    "decay = (1./0.75 - 1)/n_episode\n",
    "opt = tf.keras.optimizers.Adam(learning_rate = 1e-3, decay = decay)\n",
    "agent = Agent()\n",
    "agent.compile(opt = opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 128)               640       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 898\n",
      "Trainable params: 898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "agent.FCN().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train(n_episodes = n_episode, n_trial = n_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "... loading models ...\n",
      "game: 1, step: 500\n",
      "game: 2, step: 500\n",
      "game: 3, step: 500\n",
      "game: 4, step: 500\n",
      "game: 5, step: 383\n",
      "game: 6, step: 500\n",
      "game: 7, step: 500\n",
      "game: 8, step: 500\n",
      "game: 9, step: 500\n",
      "game: 10, step: 500\n",
      "average step: 488.3\n"
     ]
    }
   ],
   "source": [
    "#start playing\n",
    "agent.load_models()\n",
    "\n",
    "s_steps = 0\n",
    "for i in range(10):\n",
    "    step = 0\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _ = agent.select_action(observation)\n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        observation = observation_\n",
    "        env.render()\n",
    "    \n",
    "        step += 1\n",
    "    s_steps += step\n",
    "    print(f'game: {i + 1}, step: {step}')\n",
    "    \n",
    "print(f\"average step: {s_steps/10.}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
