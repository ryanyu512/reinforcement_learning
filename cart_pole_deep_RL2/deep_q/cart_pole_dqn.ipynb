{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/r/Desktop/miniforge3/envs/tf_env/lib/python3.10/site-packages/gym/envs/registration.py:423: UserWarning: \u001b[33mWARN: Custom namespace `ALE` is being overridden by namespace `ALE`. If you are developing a plugin you shouldn't specify a namespace in `register` calls. The namespace is specified through the entry point package metadata.\u001b[0m\n",
      "  logger.warn(\n",
      "Warning: Gym version v0.24.0 has a number of critical issues with `gym.make` such that the `reset` and `step` functions are called before returning the environment. It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Evironment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/r/Desktop/miniforge3/envs/tf_env/lib/python3.10/site-packages/gym/envs/registration.py:568: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env._max_episode_steps = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Experience Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experience_replay(object):\n",
    "    def __init__(self, max_size, input_shape = 4):\n",
    "        #define max memory size\n",
    "        self.mem_size = max_size \n",
    "        \n",
    "        #memory counter\n",
    "        self.mem_cntr = 0 \n",
    "        \n",
    "        #initialise state memory\n",
    "        self.state_memory = np.zeros((self.mem_size, input_shape), dtype = np.float32)\n",
    "        \n",
    "        #initialise new state memory\n",
    "        self.new_state_memory = np.zeros((self.mem_size, input_shape), dtype = np.float32)\n",
    "        \n",
    "        #initialise action memory\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype = np.int32)\n",
    "        \n",
    "        #initialise reward memory\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype = np.float32)\n",
    "        \n",
    "        #initialise terminal memory (is it done?)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.uint8)\n",
    "        \n",
    "    def push(self, state, action, reward, state_, done):\n",
    "        \n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.reward_memory[index] = reward\n",
    "        self.action_memory[index] = action\n",
    "        self.terminal_memory[index] = done\n",
    "        \n",
    "        self.mem_cntr += 1\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        #randamly sample memory in the size of batch_size \n",
    "        #replace = False => index cannot be chosen multiple times\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, batch_size, replace = False)\n",
    "        \n",
    "        #extract batch of memory\n",
    "        states = self.state_memory[batch]\n",
    "        new_states = self.new_state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        dones = self.terminal_memory[batch]\n",
    "        \n",
    "        return states, actions, rewards, new_states, dones        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DQN(n_inputs = 4, n_hiddens = [128, 64], n_actions = 2, lr = 1e-3):\n",
    "    \n",
    "    model = Sequential() \n",
    "    model.add(tf.keras.Input(shape = (n_inputs,)))\n",
    "    model.add(Dense(units = n_hiddens[0], activation = \"relu\"))\n",
    "    model.add(Dense(units = n_hiddens[1], activation = \"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units = n_actions))\n",
    "    \n",
    "    model.compile(optimizer=Adam(lr=lr), loss='mean_squared_error')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, \n",
    "                 lr = 1e-3, \n",
    "                 eps_start = 1.0, \n",
    "                 eps_end = 0.01, \n",
    "                 eps_decay = 200, \n",
    "                 batch_size = 128, \n",
    "                 target_update = 40, \n",
    "                 gamma = 0.99,\n",
    "                 n_actions = 2):\n",
    "        \n",
    "        self.experience = Experience_replay(max_size = 100000, input_shape = 4)\n",
    "        \n",
    "        self.q_eval   = DQN()\n",
    "        self.q_target = DQN()\n",
    "        self.q_target.set_weights(self.q_eval.get_weights())\n",
    "        \n",
    "        self.eps_start = eps_start\n",
    "        self.eps_decay = eps_decay\n",
    "        self.eps_end = eps_end \n",
    "        self.eps_threshold = None\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.target_update = target_update\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.action_space = [_ for _ in range(n_actions)]\n",
    "        self.step = 0\n",
    "        \n",
    "        self.training_result = {\"reward\": [], \n",
    "                                \"mean reward\": []}\n",
    "        \n",
    "        self.q_eval_model_file = 'q_eval.h5'\n",
    "        self.q_target_model_file = 'q_target.h5'\n",
    "        \n",
    "    def select_action(self, observation):\n",
    "\n",
    "        sample = random.random()\n",
    "        self.eps_threshold = self.eps_end + (self.eps_start - self.eps_end)*np.exp(-self.step/self.eps_decay)\n",
    "        \n",
    "        if sample > self.eps_threshold:\n",
    "            state = np.array([observation], copy=False, dtype = np.float32)\n",
    "            actions = self.q_eval.predict(state, verbose = 0)\n",
    "            action = np.argmax(actions)\n",
    "        else:\n",
    "            action = np.random.choice(self.action_space)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def replace_target_network(self):\n",
    "        if self.target_update != 0 and self.step % self.target_update == 0:\n",
    "            self.q_target.set_weights(self.q_eval.get_weights())\n",
    "    \n",
    "    def learn(self):\n",
    "        if self.experience.mem_cntr > self.batch_size:\n",
    "            states, actions, rewards, new_states, dones = self.experience.sample(self.batch_size)\n",
    "            \n",
    "            self.replace_target_network()\n",
    "            \n",
    "            q_eval = self.q_eval.predict(states, verbose = 0)\n",
    "            q_next = self.q_target.predict(new_states, verbose = 0)\n",
    "            q_next[dones] = 0\n",
    "            \n",
    "            indices = np.arange(self.batch_size)\n",
    "            q_target = q_eval[:]\n",
    "        \n",
    "            q_target[indices, actions] = rewards + self.gamma*np.max(q_next, axis = 1) \n",
    "\n",
    "            self.q_eval.train_on_batch(states, q_target)\n",
    "             \n",
    "            self.step += 1\n",
    "            \n",
    "    def train(self, n_episodes = 100, n_trial = 500, windows = 10):\n",
    "        steps = 0\n",
    "        sliding_reward = []\n",
    "        mean_reward = 0\n",
    "        best_reward = -np.inf\n",
    "        for episode in range(n_episodes):\n",
    "            c_samples = 0\n",
    "            rewards   = 0\n",
    "            \n",
    "            obs = env.reset()\n",
    "            for i in range(n_trial):\n",
    "                #select action\n",
    "                action = self.select_action(obs)\n",
    "                \n",
    "                #interact with the environment\n",
    "                obs_, reward, done, info = env.step(action)\n",
    "                \n",
    "                #if the agent is fail before time limit => reward = -10\n",
    "                if done and i < env._max_episode_steps - 1: \n",
    "                    reward = -10\n",
    "                \n",
    "                #store experience\n",
    "                self.experience.push(obs, action, reward, obs_, done)\n",
    "                obs = obs_\n",
    "                \n",
    "                #learn from experience\n",
    "                self.learn()\n",
    "                \n",
    "                rewards += reward\n",
    "                \n",
    "                if done:\n",
    "                    break   \n",
    "                \n",
    "                steps += 1\n",
    "                c_samples += self.batch_size\n",
    "            \n",
    "            sliding_reward.append(rewards)\n",
    "            mean_reward = np.mean(sliding_reward[-windows:])\n",
    "            \n",
    "            if mean_reward > best_reward:\n",
    "                self.save_models()\n",
    "                best_reward = mean_reward\n",
    "                \n",
    "            self.training_result[\"reward\"].append(rewards)\n",
    "            self.training_result[\"mean reward\"].append(mean_reward)\n",
    "            \n",
    "\n",
    "            print(f'episode: {episode}, rewards: {rewards}, mean reward: {mean_reward}, best reward: {best_reward}, threshold: {self.eps_threshold}')\n",
    "                \n",
    "    def save_models(self):\n",
    "        self.q_eval.save(self.q_eval_model_file)\n",
    "        self.q_target.save(self.q_target_model_file)\n",
    "        print(\"... saving model ...\")\n",
    "        \n",
    "    def load_models(self):\n",
    "        self.q_eval = load_model(self.q_eval_model_file)\n",
    "        self.q_target = load_model(self.q_target_model_file)\n",
    "        print('... loading models ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 19:47:48.396654: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-10-26 19:47:48.398106: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/r/Desktop/miniforge3/envs/tf_env/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "agent = Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... saving model ...\n",
      "episode: 0, rewards: 16.0, mean reward: 16.0, best reward: 16.0, threshold: 1.0\n",
      "episode: 1, rewards: 9.0, mean reward: 12.5, best reward: 16.0, threshold: 1.0\n",
      "episode: 2, rewards: 20.0, mean reward: 15.0, best reward: 16.0, threshold: 1.0\n",
      "episode: 3, rewards: 3.0, mean reward: 12.0, best reward: 16.0, threshold: 1.0\n",
      "episode: 4, rewards: 5.0, mean reward: 10.6, best reward: 16.0, threshold: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 01:54:56.366824: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-10-26 01:54:56.398142: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-10-26 01:54:56.470514: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-10-26 01:54:56.660005: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5, rewards: 11.0, mean reward: 10.666666666666666, best reward: 16.0, threshold: 0.9950623544007555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 01:54:57.311042: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6, rewards: 8.0, mean reward: 10.285714285714286, best reward: 16.0, threshold: 0.9057890438555999\n",
      "episode: 7, rewards: 10.0, mean reward: 10.25, best reward: 16.0, threshold: 0.8165008432473004\n",
      "episode: 8, rewards: 5.0, mean reward: 9.666666666666666, best reward: 16.0, threshold: 0.7544941117761887\n",
      "episode: 9, rewards: -1.0, mean reward: 8.6, best reward: 16.0, threshold: 0.7181847054890343\n",
      "episode: 10, rewards: 20.0, mean reward: 9.0, best reward: 16.0, threshold: 0.616500130242572\n",
      "episode: 11, rewards: 21.0, mean reward: 10.2, best reward: 16.0, threshold: 0.5268253189934059\n",
      "episode: 12, rewards: 4.0, mean reward: 8.6, best reward: 16.0, threshold: 0.48948132326580884\n",
      "episode: 13, rewards: 8.0, mean reward: 9.1, best reward: 16.0, threshold: 0.4460273379609393\n",
      "episode: 14, rewards: 30.0, mean reward: 11.6, best reward: 16.0, threshold: 0.3652085007518921\n",
      "... saving model ...\n",
      "episode: 15, rewards: 94.0, mean reward: 19.9, best reward: 19.9, threshold: 0.22012549408847562\n",
      "... saving model ...\n",
      "episode: 16, rewards: 370.0, mean reward: 56.1, best reward: 56.1, threshold: 0.04127143131252988\n",
      "... saving model ...\n",
      "episode: 17, rewards: 195.0, mean reward: 74.6, best reward: 74.6, threshold: 0.02116411864553315\n",
      "... saving model ...\n",
      "episode: 18, rewards: 281.0, mean reward: 102.2, best reward: 102.2, threshold: 0.01259271332487966\n",
      "... saving model ...\n",
      "episode: 19, rewards: 294.0, mean reward: 131.7, best reward: 131.7, threshold: 0.010564229013908884\n",
      "... saving model ...\n",
      "episode: 20, rewards: 204.0, mean reward: 150.1, best reward: 150.1, threshold: 0.010192570095922796\n",
      "... saving model ...\n",
      "episode: 21, rewards: 185.0, mean reward: 166.5, best reward: 166.5, threshold: 0.010072273694306704\n",
      "... saving model ...\n",
      "episode: 22, rewards: 211.0, mean reward: 187.2, best reward: 187.2, threshold: 0.010023818443608784\n",
      "... saving model ...\n",
      "episode: 23, rewards: 156.0, mean reward: 202.0, best reward: 202.0, threshold: 0.010010334214869302\n",
      "... saving model ...\n",
      "episode: 24, rewards: 287.0, mean reward: 227.7, best reward: 227.7, threshold: 0.01000232904944801\n",
      "... saving model ...\n",
      "episode: 25, rewards: 370.0, mean reward: 255.3, best reward: 255.3, threshold: 0.010000346615293651\n",
      "episode: 26, rewards: 219.0, mean reward: 240.2, best reward: 255.3, threshold: 0.0100001097511468\n",
      "episode: 27, rewards: 229.0, mean reward: 243.6, best reward: 255.3, threshold: 0.010000033056410167\n",
      "... saving model ...\n",
      "episode: 28, rewards: 414.0, mean reward: 256.9, best reward: 256.9, threshold: 0.010000003948025186\n",
      "episode: 29, rewards: 175.0, mean reward: 245.0, best reward: 256.9, threshold: 0.010000001557707985\n",
      "... saving model ...\n",
      "episode: 30, rewards: 500.0, mean reward: 274.6, best reward: 274.6, threshold: 0.010000000127864459\n",
      "... saving model ...\n",
      "episode: 31, rewards: 188.0, mean reward: 274.9, best reward: 274.9, threshold: 0.010000000047274488\n",
      "... saving model ...\n",
      "episode: 32, rewards: 252.0, mean reward: 279.0, best reward: 279.0, threshold: 0.010000000012691987\n",
      "episode: 33, rewards: 135.0, mean reward: 276.9, best reward: 279.0, threshold: 0.010000000006116382\n",
      "episode: 34, rewards: 132.0, mean reward: 261.4, best reward: 279.0, threshold: 0.010000000002992086\n",
      "episode: 35, rewards: 164.0, mean reward: 240.8, best reward: 279.0, threshold: 0.010000000001247287\n",
      "episode: 36, rewards: 323.0, mean reward: 251.2, best reward: 279.0, threshold: 0.010000000000234799\n",
      "episode: 37, rewards: 235.0, mean reward: 251.8, best reward: 279.0, threshold: 0.01000000000006863\n",
      "episode: 38, rewards: 166.0, mean reward: 227.0, best reward: 279.0, threshold: 0.010000000000028325\n",
      "episode: 39, rewards: 500.0, mean reward: 259.5, best reward: 279.0, threshold: 0.010000000000002325\n",
      "episode: 40, rewards: 218.0, mean reward: 231.3, best reward: 279.0, threshold: 0.010000000000000741\n",
      "episode: 41, rewards: 180.0, mean reward: 230.5, best reward: 279.0, threshold: 0.010000000000000285\n",
      "episode: 42, rewards: 481.0, mean reward: 253.4, best reward: 279.0, threshold: 0.010000000000000024\n",
      "episode: 43, rewards: 352.0, mean reward: 275.1, best reward: 279.0, threshold: 0.010000000000000004\n",
      "episode: 44, rewards: 159.0, mean reward: 277.8, best reward: 279.0, threshold: 0.010000000000000002\n",
      "... saving model ...\n",
      "episode: 45, rewards: 218.0, mean reward: 283.2, best reward: 283.2, threshold: 0.01\n",
      "episode: 46, rewards: 149.0, mean reward: 265.8, best reward: 283.2, threshold: 0.01\n",
      "... saving model ...\n",
      "episode: 47, rewards: 427.0, mean reward: 285.0, best reward: 285.0, threshold: 0.01\n",
      "episode: 48, rewards: 147.0, mean reward: 283.1, best reward: 285.0, threshold: 0.01\n",
      "episode: 49, rewards: 500.0, mean reward: 283.1, best reward: 285.0, threshold: 0.01\n",
      "episode: 50, rewards: 141.0, mean reward: 275.4, best reward: 285.0, threshold: 0.01\n",
      "episode: 51, rewards: 152.0, mean reward: 272.6, best reward: 285.0, threshold: 0.01\n",
      "episode: 52, rewards: 136.0, mean reward: 238.1, best reward: 285.0, threshold: 0.01\n",
      "episode: 53, rewards: 500.0, mean reward: 252.9, best reward: 285.0, threshold: 0.01\n",
      "episode: 54, rewards: 328.0, mean reward: 269.8, best reward: 285.0, threshold: 0.01\n",
      "episode: 55, rewards: 176.0, mean reward: 265.6, best reward: 285.0, threshold: 0.01\n",
      "episode: 56, rewards: 327.0, mean reward: 283.4, best reward: 285.0, threshold: 0.01\n",
      "... saving model ...\n",
      "episode: 57, rewards: 500.0, mean reward: 290.7, best reward: 290.7, threshold: 0.01\n",
      "... saving model ...\n",
      "episode: 58, rewards: 314.0, mean reward: 307.4, best reward: 307.4, threshold: 0.01\n",
      "episode: 59, rewards: 500.0, mean reward: 307.4, best reward: 307.4, threshold: 0.01\n",
      "... saving model ...\n",
      "episode: 60, rewards: 500.0, mean reward: 343.3, best reward: 343.3, threshold: 0.01\n",
      "... saving model ...\n",
      "episode: 61, rewards: 262.0, mean reward: 354.3, best reward: 354.3, threshold: 0.01\n",
      "... saving model ...\n",
      "episode: 62, rewards: 479.0, mean reward: 388.6, best reward: 388.6, threshold: 0.01\n",
      "episode: 63, rewards: 475.0, mean reward: 386.1, best reward: 388.6, threshold: 0.01\n",
      "episode: 64, rewards: 227.0, mean reward: 376.0, best reward: 388.6, threshold: 0.01\n",
      "episode: 65, rewards: 227.0, mean reward: 381.1, best reward: 388.6, threshold: 0.01\n",
      "episode: 66, rewards: 148.0, mean reward: 363.2, best reward: 388.6, threshold: 0.01\n",
      "episode: 67, rewards: 206.0, mean reward: 333.8, best reward: 388.6, threshold: 0.01\n",
      "episode: 68, rewards: 500.0, mean reward: 352.4, best reward: 388.6, threshold: 0.01\n",
      "episode: 69, rewards: 188.0, mean reward: 321.2, best reward: 388.6, threshold: 0.01\n",
      "episode: 70, rewards: 255.0, mean reward: 296.7, best reward: 388.6, threshold: 0.01\n",
      "episode: 71, rewards: 414.0, mean reward: 311.9, best reward: 388.6, threshold: 0.01\n",
      "episode: 72, rewards: 147.0, mean reward: 278.7, best reward: 388.6, threshold: 0.01\n",
      "episode: 73, rewards: 435.0, mean reward: 274.7, best reward: 388.6, threshold: 0.01\n",
      "episode: 74, rewards: 203.0, mean reward: 272.3, best reward: 388.6, threshold: 0.01\n",
      "episode: 75, rewards: 252.0, mean reward: 274.8, best reward: 388.6, threshold: 0.01\n",
      "episode: 76, rewards: 156.0, mean reward: 275.6, best reward: 388.6, threshold: 0.01\n",
      "episode: 77, rewards: 233.0, mean reward: 278.3, best reward: 388.6, threshold: 0.01\n",
      "episode: 78, rewards: 158.0, mean reward: 244.1, best reward: 388.6, threshold: 0.01\n",
      "episode: 79, rewards: 211.0, mean reward: 246.4, best reward: 388.6, threshold: 0.01\n",
      "episode: 80, rewards: 150.0, mean reward: 235.9, best reward: 388.6, threshold: 0.01\n",
      "episode: 81, rewards: 500.0, mean reward: 244.5, best reward: 388.6, threshold: 0.01\n",
      "episode: 82, rewards: 215.0, mean reward: 251.3, best reward: 388.6, threshold: 0.01\n",
      "episode: 83, rewards: 393.0, mean reward: 247.1, best reward: 388.6, threshold: 0.01\n",
      "episode: 84, rewards: 265.0, mean reward: 253.3, best reward: 388.6, threshold: 0.01\n",
      "episode: 85, rewards: 158.0, mean reward: 243.9, best reward: 388.6, threshold: 0.01\n",
      "episode: 86, rewards: 500.0, mean reward: 278.3, best reward: 388.6, threshold: 0.01\n",
      "episode: 87, rewards: 220.0, mean reward: 277.0, best reward: 388.6, threshold: 0.01\n",
      "episode: 88, rewards: 368.0, mean reward: 298.0, best reward: 388.6, threshold: 0.01\n",
      "episode: 89, rewards: 384.0, mean reward: 315.3, best reward: 388.6, threshold: 0.01\n",
      "episode: 90, rewards: 242.0, mean reward: 324.5, best reward: 388.6, threshold: 0.01\n",
      "episode: 91, rewards: 458.0, mean reward: 320.3, best reward: 388.6, threshold: 0.01\n",
      "episode: 92, rewards: 277.0, mean reward: 326.5, best reward: 388.6, threshold: 0.01\n",
      "episode: 93, rewards: 480.0, mean reward: 335.2, best reward: 388.6, threshold: 0.01\n",
      "episode: 94, rewards: 202.0, mean reward: 328.9, best reward: 388.6, threshold: 0.01\n",
      "episode: 95, rewards: 345.0, mean reward: 347.6, best reward: 388.6, threshold: 0.01\n",
      "episode: 96, rewards: 234.0, mean reward: 321.0, best reward: 388.6, threshold: 0.01\n",
      "episode: 97, rewards: 137.0, mean reward: 312.7, best reward: 388.6, threshold: 0.01\n",
      "episode: 98, rewards: 154.0, mean reward: 291.3, best reward: 388.6, threshold: 0.01\n",
      "episode: 99, rewards: 218.0, mean reward: 274.7, best reward: 388.6, threshold: 0.01\n",
      "episode: 100, rewards: 148.0, mean reward: 265.3, best reward: 388.6, threshold: 0.01\n",
      "episode: 101, rewards: 500.0, mean reward: 269.5, best reward: 388.6, threshold: 0.01\n",
      "episode: 102, rewards: 424.0, mean reward: 284.2, best reward: 388.6, threshold: 0.01\n",
      "episode: 103, rewards: 500.0, mean reward: 286.2, best reward: 388.6, threshold: 0.01\n",
      "episode: 104, rewards: 222.0, mean reward: 288.2, best reward: 388.6, threshold: 0.01\n",
      "episode: 105, rewards: 500.0, mean reward: 303.7, best reward: 388.6, threshold: 0.01\n",
      "episode: 106, rewards: 364.0, mean reward: 316.7, best reward: 388.6, threshold: 0.01\n",
      "episode: 107, rewards: 157.0, mean reward: 318.7, best reward: 388.6, threshold: 0.01\n",
      "episode: 108, rewards: 411.0, mean reward: 344.4, best reward: 388.6, threshold: 0.01\n",
      "episode: 109, rewards: 189.0, mean reward: 341.5, best reward: 388.6, threshold: 0.01\n",
      "episode: 110, rewards: 500.0, mean reward: 376.7, best reward: 388.6, threshold: 0.01\n",
      "episode: 111, rewards: 154.0, mean reward: 342.1, best reward: 388.6, threshold: 0.01\n",
      "episode: 112, rewards: 415.0, mean reward: 341.2, best reward: 388.6, threshold: 0.01\n",
      "episode: 113, rewards: 171.0, mean reward: 308.3, best reward: 388.6, threshold: 0.01\n",
      "episode: 114, rewards: 164.0, mean reward: 302.5, best reward: 388.6, threshold: 0.01\n",
      "episode: 115, rewards: 140.0, mean reward: 266.5, best reward: 388.6, threshold: 0.01\n",
      "episode: 116, rewards: 500.0, mean reward: 280.1, best reward: 388.6, threshold: 0.01\n",
      "episode: 117, rewards: 464.0, mean reward: 310.8, best reward: 388.6, threshold: 0.01\n",
      "episode: 118, rewards: 177.0, mean reward: 287.4, best reward: 388.6, threshold: 0.01\n",
      "episode: 119, rewards: 388.0, mean reward: 307.3, best reward: 388.6, threshold: 0.01\n",
      "episode: 120, rewards: 500.0, mean reward: 307.3, best reward: 388.6, threshold: 0.01\n",
      "episode: 121, rewards: 256.0, mean reward: 317.5, best reward: 388.6, threshold: 0.01\n",
      "episode: 122, rewards: 212.0, mean reward: 297.2, best reward: 388.6, threshold: 0.01\n",
      "episode: 123, rewards: 142.0, mean reward: 294.3, best reward: 388.6, threshold: 0.01\n",
      "episode: 124, rewards: 406.0, mean reward: 318.5, best reward: 388.6, threshold: 0.01\n",
      "episode: 125, rewards: 162.0, mean reward: 320.7, best reward: 388.6, threshold: 0.01\n",
      "episode: 126, rewards: 500.0, mean reward: 320.7, best reward: 388.6, threshold: 0.01\n",
      "episode: 127, rewards: 139.0, mean reward: 288.2, best reward: 388.6, threshold: 0.01\n",
      "episode: 128, rewards: 310.0, mean reward: 301.5, best reward: 388.6, threshold: 0.01\n",
      "episode: 129, rewards: 97.0, mean reward: 272.4, best reward: 388.6, threshold: 0.01\n",
      "episode: 130, rewards: 170.0, mean reward: 239.4, best reward: 388.6, threshold: 0.01\n",
      "episode: 131, rewards: 151.0, mean reward: 228.9, best reward: 388.6, threshold: 0.01\n",
      "episode: 132, rewards: 394.0, mean reward: 247.1, best reward: 388.6, threshold: 0.01\n",
      "episode: 133, rewards: 278.0, mean reward: 260.7, best reward: 388.6, threshold: 0.01\n",
      "episode: 134, rewards: 209.0, mean reward: 241.0, best reward: 388.6, threshold: 0.01\n",
      "episode: 135, rewards: 389.0, mean reward: 263.7, best reward: 388.6, threshold: 0.01\n",
      "episode: 136, rewards: 164.0, mean reward: 230.1, best reward: 388.6, threshold: 0.01\n",
      "episode: 137, rewards: 160.0, mean reward: 232.2, best reward: 388.6, threshold: 0.01\n",
      "episode: 138, rewards: 334.0, mean reward: 234.6, best reward: 388.6, threshold: 0.01\n",
      "episode: 139, rewards: 348.0, mean reward: 259.7, best reward: 388.6, threshold: 0.01\n",
      "episode: 140, rewards: 148.0, mean reward: 257.5, best reward: 388.6, threshold: 0.01\n",
      "episode: 141, rewards: 500.0, mean reward: 292.4, best reward: 388.6, threshold: 0.01\n",
      "episode: 142, rewards: 143.0, mean reward: 267.3, best reward: 388.6, threshold: 0.01\n",
      "episode: 143, rewards: 500.0, mean reward: 289.5, best reward: 388.6, threshold: 0.01\n",
      "episode: 144, rewards: 232.0, mean reward: 291.8, best reward: 388.6, threshold: 0.01\n",
      "episode: 145, rewards: 224.0, mean reward: 275.3, best reward: 388.6, threshold: 0.01\n",
      "episode: 146, rewards: 236.0, mean reward: 282.5, best reward: 388.6, threshold: 0.01\n",
      "episode: 147, rewards: 188.0, mean reward: 285.3, best reward: 388.6, threshold: 0.01\n",
      "episode: 148, rewards: 157.0, mean reward: 267.6, best reward: 388.6, threshold: 0.01\n",
      "episode: 149, rewards: 212.0, mean reward: 254.0, best reward: 388.6, threshold: 0.01\n",
      "episode: 150, rewards: 468.0, mean reward: 286.0, best reward: 388.6, threshold: 0.01\n",
      "episode: 151, rewards: 158.0, mean reward: 251.8, best reward: 388.6, threshold: 0.01\n",
      "episode: 152, rewards: 148.0, mean reward: 252.3, best reward: 388.6, threshold: 0.01\n",
      "episode: 153, rewards: 334.0, mean reward: 235.7, best reward: 388.6, threshold: 0.01\n",
      "episode: 154, rewards: 189.0, mean reward: 231.4, best reward: 388.6, threshold: 0.01\n",
      "episode: 155, rewards: 151.0, mean reward: 224.1, best reward: 388.6, threshold: 0.01\n",
      "episode: 156, rewards: 144.0, mean reward: 214.9, best reward: 388.6, threshold: 0.01\n",
      "episode: 157, rewards: 258.0, mean reward: 221.9, best reward: 388.6, threshold: 0.01\n",
      "episode: 158, rewards: 369.0, mean reward: 243.1, best reward: 388.6, threshold: 0.01\n",
      "episode: 159, rewards: 197.0, mean reward: 241.6, best reward: 388.6, threshold: 0.01\n",
      "episode: 160, rewards: 175.0, mean reward: 212.3, best reward: 388.6, threshold: 0.01\n",
      "episode: 161, rewards: 258.0, mean reward: 222.3, best reward: 388.6, threshold: 0.01\n",
      "episode: 162, rewards: 430.0, mean reward: 250.5, best reward: 388.6, threshold: 0.01\n",
      "episode: 163, rewards: 484.0, mean reward: 265.5, best reward: 388.6, threshold: 0.01\n",
      "episode: 164, rewards: 211.0, mean reward: 267.7, best reward: 388.6, threshold: 0.01\n",
      "episode: 165, rewards: 146.0, mean reward: 267.2, best reward: 388.6, threshold: 0.01\n",
      "episode: 166, rewards: 245.0, mean reward: 277.3, best reward: 388.6, threshold: 0.01\n",
      "episode: 167, rewards: 253.0, mean reward: 276.8, best reward: 388.6, threshold: 0.01\n",
      "episode: 168, rewards: 191.0, mean reward: 259.0, best reward: 388.6, threshold: 0.01\n",
      "episode: 169, rewards: 162.0, mean reward: 255.5, best reward: 388.6, threshold: 0.01\n",
      "episode: 170, rewards: 500.0, mean reward: 288.0, best reward: 388.6, threshold: 0.01\n",
      "episode: 171, rewards: 167.0, mean reward: 278.9, best reward: 388.6, threshold: 0.01\n",
      "episode: 172, rewards: 374.0, mean reward: 273.3, best reward: 388.6, threshold: 0.01\n",
      "episode: 173, rewards: 262.0, mean reward: 251.1, best reward: 388.6, threshold: 0.01\n",
      "episode: 174, rewards: 243.0, mean reward: 254.3, best reward: 388.6, threshold: 0.01\n",
      "episode: 175, rewards: 159.0, mean reward: 255.6, best reward: 388.6, threshold: 0.01\n",
      "episode: 176, rewards: 325.0, mean reward: 263.6, best reward: 388.6, threshold: 0.01\n",
      "episode: 177, rewards: 500.0, mean reward: 288.3, best reward: 388.6, threshold: 0.01\n",
      "episode: 178, rewards: 364.0, mean reward: 305.6, best reward: 388.6, threshold: 0.01\n",
      "episode: 179, rewards: 205.0, mean reward: 309.9, best reward: 388.6, threshold: 0.01\n",
      "episode: 180, rewards: 271.0, mean reward: 287.0, best reward: 388.6, threshold: 0.01\n",
      "episode: 181, rewards: 500.0, mean reward: 320.3, best reward: 388.6, threshold: 0.01\n",
      "episode: 182, rewards: 300.0, mean reward: 312.9, best reward: 388.6, threshold: 0.01\n",
      "episode: 183, rewards: 500.0, mean reward: 336.7, best reward: 388.6, threshold: 0.01\n",
      "episode: 184, rewards: 171.0, mean reward: 329.5, best reward: 388.6, threshold: 0.01\n",
      "episode: 185, rewards: 168.0, mean reward: 330.4, best reward: 388.6, threshold: 0.01\n",
      "episode: 186, rewards: 372.0, mean reward: 335.1, best reward: 388.6, threshold: 0.01\n",
      "episode: 187, rewards: 157.0, mean reward: 300.8, best reward: 388.6, threshold: 0.01\n",
      "episode: 188, rewards: 123.0, mean reward: 276.7, best reward: 388.6, threshold: 0.01\n",
      "episode: 189, rewards: 272.0, mean reward: 283.4, best reward: 388.6, threshold: 0.01\n",
      "episode: 190, rewards: 500.0, mean reward: 306.3, best reward: 388.6, threshold: 0.01\n",
      "episode: 191, rewards: 185.0, mean reward: 274.8, best reward: 388.6, threshold: 0.01\n",
      "episode: 192, rewards: 280.0, mean reward: 272.8, best reward: 388.6, threshold: 0.01\n",
      "episode: 193, rewards: 252.0, mean reward: 248.0, best reward: 388.6, threshold: 0.01\n",
      "episode: 194, rewards: 260.0, mean reward: 256.9, best reward: 388.6, threshold: 0.01\n",
      "episode: 195, rewards: 165.0, mean reward: 256.6, best reward: 388.6, threshold: 0.01\n",
      "episode: 196, rewards: 182.0, mean reward: 237.6, best reward: 388.6, threshold: 0.01\n",
      "episode: 197, rewards: 188.0, mean reward: 240.7, best reward: 388.6, threshold: 0.01\n",
      "episode: 198, rewards: 323.0, mean reward: 260.7, best reward: 388.6, threshold: 0.01\n",
      "episode: 199, rewards: 500.0, mean reward: 283.5, best reward: 388.6, threshold: 0.01\n"
     ]
    }
   ],
   "source": [
    "agent.train(n_episodes = 200, n_trial = env._max_episode_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading models ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 19:54:46.190658: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game: 1, step: 500\n",
      "game: 2, step: 500\n",
      "game: 3, step: 500\n",
      "game: 4, step: 164\n",
      "game: 5, step: 214\n",
      "game: 6, step: 163\n",
      "game: 7, step: 164\n",
      "game: 8, step: 196\n",
      "game: 9, step: 182\n",
      "game: 10, step: 500\n",
      "average step: 308.3\n"
     ]
    }
   ],
   "source": [
    "#start playing\n",
    "agent.eps_threshold = 0.00\n",
    "agent.step = 1e5\n",
    "agent.load_models()\n",
    "\n",
    "s_steps = 0\n",
    "for i in range(10):\n",
    "    step = 0\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.select_action(observation)\n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        observation = observation_\n",
    "        env.render()\n",
    "    \n",
    "        step += 1\n",
    "    s_steps += step\n",
    "    print(f'game: {i + 1}, step: {step}')\n",
    "    \n",
    "print(f\"average step: {s_steps/10.}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
