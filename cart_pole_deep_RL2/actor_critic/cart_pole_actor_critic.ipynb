{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/r/Desktop/miniforge3/envs/tf_env/lib/python3.10/site-packages/gym/envs/registration.py:423: UserWarning: \u001b[33mWARN: Custom namespace `ALE` is being overridden by namespace `ALE`. If you are developing a plugin you shouldn't specify a namespace in `register` calls. The namespace is specified through the entry point package metadata.\u001b[0m\n",
      "  logger.warn(\n",
      "Warning: Gym version v0.24.0 has a number of critical issues with `gym.make` such that the `reset` and `step` functions are called before returning the environment. It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define ActorCritic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(keras.Model):\n",
    "    def __init__(self, n_actions, h_dims = [128, 64], filename = 'actor'):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.h_dims = h_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.filename = filename\n",
    "        \n",
    "        self.d1 = Dense(self.h_dims[0], activation = 'relu')\n",
    "        self.d2 = Dense(self.h_dims[1], activation = 'relu')\n",
    "        self.p_out = Dense(n_actions, activation = 'softmax')\n",
    "        \n",
    "    def call(self, state):\n",
    "        x = self.d1(state)\n",
    "        x = self.d2(x)\n",
    "        \n",
    "        p = self.p_out(x)\n",
    "        \n",
    "        return p\n",
    "    \n",
    "class CriticNetwork(keras.Model):\n",
    "    def __init__(self, h_dims = [128, 64], filename = 'critic'):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.h_dims = h_dims\n",
    "        self.filename = filename\n",
    "        \n",
    "        self.d1 = Dense(self.h_dims[0], activation = 'relu')\n",
    "        self.d2 = Dense(self.h_dims[1], activation = 'relu')\n",
    "        self.v_out = Dense(1, activation = None)\n",
    "        \n",
    "    def call(self, state):\n",
    "        x = self.d1(state)\n",
    "        x = self.d2(x)\n",
    "        v = self.v_out(x)\n",
    "        \n",
    "        return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_frames_as_gif(frames, path='./', filename='gym_animation.gif'):\n",
    "    \n",
    "    #Mess with this to change frame size\n",
    "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi=72)\n",
    "\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    anim.save(path + filename, writer='imagemagick', fps=60)\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, \n",
    "                 lr = 1e-3, \n",
    "                 gamma = 0.99,\n",
    "                 n_actions = 2, \n",
    "                 **kwargs):\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.n_actions = n_actions\n",
    "        self.action = None\n",
    "        self.action_space = [_ for _ in range(n_actions)]\n",
    "        \n",
    "        self.a_net  = ActorNetwork(n_actions = n_actions)\n",
    "        self.a_net.compile(optimizer = Adam(learning_rate = self.lr))\n",
    "        self.c_net  = CriticNetwork()\n",
    "        self.c_net.compile(optimizer = Adam(learning_rate = self.lr))\n",
    "        \n",
    "        self.step = 0\n",
    "    \n",
    "    def select_action(self, observation, is_training = True):\n",
    "        state = tf.convert_to_tensor(np.array([observation], copy=False, dtype = np.float32))\n",
    "        \n",
    "        act_dist = self.a_net(state, training = is_training)\n",
    "        \n",
    "        act_sample = tfp.distributions.Categorical(probs = act_dist)\n",
    "        self.action = act_sample.sample()\n",
    "\n",
    "        return self.action.numpy()[0]\n",
    "        \n",
    "    def learn(self, c_state, reward, n_state, done):\n",
    "        \n",
    "        c_state = tf.convert_to_tensor([c_state], dtype = tf.float32)\n",
    "        n_state = tf.convert_to_tensor([n_state], dtype = tf.float32)\n",
    "        reward  = tf.convert_to_tensor(reward, dtype = tf.float32)\n",
    "\n",
    "        with tf.GradientTape() as a_tape, tf.GradientTape() as c_tape:\n",
    "            p     = self.a_net(c_state)\n",
    "            c_val = self.c_net(c_state)\n",
    "            n_val = self.c_net(n_state)\n",
    "            c_val = tf.squeeze(c_val)\n",
    "            n_val = tf.squeeze(n_val)\n",
    "            \n",
    "            act_p = tfp.distributions.Categorical(probs = p)\n",
    "            log_p = act_p.log_prob(self.action)\n",
    "            \n",
    "            #c_val = estimated value \n",
    "            #reward + self.gamma*n_val*(1 - done) = better estimated value = target\n",
    "            #we hope that target - c_val => 0 => indicate better estimate\n",
    "            #so, we minimise the c_loss\n",
    "            \n",
    "            delta = reward + self.gamma*n_val*(1 - done) - c_val\n",
    "            a_loss = -log_p*delta\n",
    "            c_loss = delta**2\n",
    "            \n",
    "            total_loss = a_loss + c_loss\n",
    "            \n",
    "        a_gradient = a_tape.gradient(total_loss, self.a_net.trainable_variables)\n",
    "        c_gradient = c_tape.gradient(total_loss, self.c_net.trainable_variables)\n",
    "        self.a_net.optimizer.apply_gradients(zip(a_gradient, \n",
    "                                                 self.a_net.trainable_variables))\n",
    "        self.c_net.optimizer.apply_gradients(zip(c_gradient, \n",
    "                                                 self.c_net.trainable_variables))\n",
    "\n",
    "    def train(self, n_episodes = 1800, env_max_step = 500, windows = 20, is_load_model = False):\n",
    "        \n",
    "        #setup environment\n",
    "        env = gym.make('CartPole-v0')\n",
    "        env._max_episode_steps = env_max_step\n",
    "        \n",
    "        score_hist = []\n",
    "        score_avg_hist = []        \n",
    "        score_best = -np.inf\n",
    "        \n",
    "        if is_load_model:\n",
    "            self.load_models()\n",
    "            \n",
    "        for episode in range(n_episodes):\n",
    "            \n",
    "            obs = env.reset()\n",
    "            rewards = []\n",
    "            values = []\n",
    "            \n",
    "            score = 0\n",
    "            done = False\n",
    "            step = 0\n",
    "            while not done:\n",
    "                #select action\n",
    "                action = self.select_action(obs)\n",
    "                \n",
    "                #interact with the environment\n",
    "                obs_, reward, done, info = env.step(action)\n",
    "                \n",
    "                #update score \n",
    "                score += reward\n",
    "                \n",
    "                #learn               \n",
    "                self.learn(obs, reward, obs_, done)\n",
    "                \n",
    "                #update observation\n",
    "                obs = obs_\n",
    "                \n",
    "                step += 1\n",
    "            \n",
    "            score_hist.append(score)\n",
    "            score_avg_hist.append(np.mean(score_hist[-windows:]))\n",
    "            \n",
    "            if score_avg_hist[-1] > score_best and score > score_best and episode > windows:\n",
    "                score_best = score_avg_hist[-1]\n",
    "                self.save_models()\n",
    "            \n",
    "            print(f'epidsode: {episode}, score: {score}, avg_score: {score_avg_hist[-1]}, best_score: {score_best}')\n",
    "\n",
    "        env.close()\n",
    "        \n",
    "    def demo(self, env_max_step = 500, is_save_gif = False):\n",
    "        #setup environment\n",
    "        env = gym.make('CartPole-v0')\n",
    "        env._max_episode_steps = env_max_step\n",
    "        \n",
    "        #reset environment \n",
    "        obs = env.reset()\n",
    "    \n",
    "        score = 0\n",
    "        done = False\n",
    "        \n",
    "        #load model\n",
    "        self.load_models()\n",
    "        \n",
    "        frames = []\n",
    "        while not done:\n",
    "            #select action\n",
    "            action = self.select_action(obs, is_training = False)\n",
    "            \n",
    "            #interact with the environment\n",
    "            obs_, reward, done, info = env.step(action)\n",
    "            \n",
    "            #update score \n",
    "            score += reward\n",
    "            \n",
    "            #update observation\n",
    "            obs = obs_\n",
    "            if is_save_gif:\n",
    "                frames.append(env.render(mode=\"rgb_array\"))\n",
    "            else:\n",
    "                env.render()\n",
    "\n",
    "        print(f'score: {score}')\n",
    "        env.close()\n",
    "        save_frames_as_gif(frames)\n",
    "        \n",
    "    def save_models(self):\n",
    "        self.a_net.save_weights(self.a_net.filename)\n",
    "        self.c_net.save_weights(self.c_net.filename)\n",
    "        print(\"... saving model ...\")\n",
    "        \n",
    "    def load_models(self):\n",
    "        self.a_net.load_weights(self.a_net.filename)\n",
    "        self.c_net.load_weights(self.c_net.filename)\n",
    "        print('... loading models ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:24:39.896227: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-11-08 17:24:39.896476: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "n_episode = 500\n",
    "agent = Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train(n_episodes = n_episode, env_max_step = 500, is_load_model = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/r/Desktop/miniforge3/envs/tf_env/lib/python3.10/site-packages/gym/envs/registration.py:568: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading models ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MovieWriter imagemagick unavailable; using Pillow instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 500.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent.demo(is_save_gif = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
