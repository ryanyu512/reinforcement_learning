{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game import *\n",
    "from agent import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(epsilon = 1e-10)\n",
    "agent.iterate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly put some state for checking if the policy is reasonable\n",
    "s = (2,2,1,\n",
    "     0,1,0,\n",
    "     0,0,0)\n",
    "\n",
    "#the following code is for debug\n",
    "actions_agent2, next_states_agent2 = agent.gen_next_actions_states(AGENT2, s)  \n",
    "\n",
    "old_v, agent.Q[s] = agent.V[s], [0]*agent.n_grid\n",
    "for a2, ns2 in zip(actions_agent2, next_states_agent2):\n",
    "     print(f\"=== action {a2} ===\")\n",
    "     #update self.V[ns2] based on agent1's possible actions and next states\n",
    "     agent.V[ns2] = 0\n",
    "     actions_agent1, next_states_agent1 = agent.gen_next_actions_states(AGENT1, ns2)  \n",
    "     for a1, ns1 in zip(actions_agent1, next_states_agent1):\n",
    "          agent.V[ns2] += 1/len(actions_agent1)*agent.get_transition_p(ns2, actions_agent1)*(agent.get_reward(ns1) + agent.gamma*agent.V[ns1])\n",
    "          print(ns1, ' r: ', agent.get_reward(ns1), ' agent.V[ns1]: ', agent.V[ns1])\n",
    "     # print('ns2:', ns2)\n",
    "     print('agent.V[ns2]: ', agent.V[ns2])\n",
    "     # #update self.Q[cs][a2]\n",
    "     agent.Q[s][a2] = agent.get_transition_p(s, actions_agent2)*(agent.get_reward(ns2) + agent.gamma*agent.V[ns2])\n",
    "     print('agent.Q[s][a2]: ', agent.Q[s][a2])\n",
    "\n",
    "# print(\"is_invalid: \", agent.is_invalid_state(s, AGENT2))\n",
    "print(f'policy: {agent.policy[s]}')\n",
    "# print(f'Q: {agent.Q[s]}')\n",
    "# print('next actions: ', actions)\n",
    "\n",
    "# for i in range(len(next_states)):\n",
    "#      print('next_states: ', next_states[i])\n",
    "#      print(f'next V: ', agent.V[next_states[i]], 'action: ', actions[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robot_msc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
